{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54340229",
   "metadata": {},
   "source": [
    "# Web Scraping with Python\n",
    "\n",
    "This notebook guides you through the process of web scraping using Python. We'll use common libraries to fetch, parse, and extract data from websites.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Web Scraping](#Introduction-to-Web-Scraping)\n",
    "2. [Required Libraries](#Import-Required-Libraries)\n",
    "3. [Ethical Web Scraping](#Ethical-Web-Scraping)\n",
    "4. [HTTP Basics](#HTTP-Basics)\n",
    "5. [Fetch HTML Content](#Fetch-HTML-Content)\n",
    "6. [Parse HTML with BeautifulSoup](#Parse-HTML-with-BeautifulSoup)\n",
    "7. [Extract Specific Data](#Extract-Specific-Data)\n",
    "8. [Handle Pagination](#Handle-Pagination)\n",
    "9. [Structured Data: Scraping Tables](#Structured-Data-Scraping-Tables)\n",
    "10. [Dynamic Websites and JavaScript](#Dynamic-Websites-and-JavaScript)\n",
    "11. [Error Handling](#Error-Handling-and-Robust-Scraping)\n",
    "12. [Practical Exercises](#Practical-Exercises)\n",
    "13. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0c48f",
   "metadata": {},
   "source": [
    "## Introduction to Web Scraping\n",
    "\n",
    "Web scraping is the process of programmatically extracting data from websites. It's useful when you need to:\n",
    "\n",
    "- Collect data that's not available through an API\n",
    "- Monitor websites for changes\n",
    "- Aggregate information from multiple sources\n",
    "- Create datasets for analysis or machine learning\n",
    "\n",
    "In this notebook, we'll cover the fundamentals of web scraping with Python, from making HTTP requests to parsing HTML and extracting useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4584629",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import libraries such as requests and BeautifulSoup for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9b00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a983ff",
   "metadata": {},
   "source": [
    "## Ethical Web Scraping\n",
    "\n",
    "Before we start scraping websites, it's important to understand the legal and ethical considerations involved:\n",
    "\n",
    "### Legal and Ethical Considerations\n",
    "\n",
    "1. **Terms of Service (ToS)**: Always review a website's Terms of Service. Many websites explicitly prohibit scraping.\n",
    "2. **Copyright Laws**: Data on websites may be protected by copyright. Scraping and republishing it could be a violation.\n",
    "3. **Rate Limiting**: Sending too many requests too quickly can overload a website's servers, which is effectively a DDoS attack.\n",
    "4. **Personal Data**: Scraping personal data may violate privacy laws like GDPR or CCPA.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Check robots.txt**: This file tells web crawlers what parts of the site they can access.\n",
    "2. **Use Delays**: Add time delays between requests to reduce server load.\n",
    "3. **Identify Your Bot**: Set a proper User-Agent header that identifies your scraper.\n",
    "4. **Cache Results**: Don't scrape the same content repeatedly.\n",
    "5. **Use APIs When Available**: If a website offers an API, use it instead of scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de07eb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No robots.txt found at https://quotes.toscrape.com/robots.txt\n"
     ]
    }
   ],
   "source": [
    "# Example: How to check robots.txt before scraping\n",
    "import requests\n",
    "\n",
    "\n",
    "def check_robots_txt(url):\n",
    "    # Extract the base URL\n",
    "    from urllib.parse import urlparse\n",
    "\n",
    "    parsed_url = urlparse(url)\n",
    "    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "\n",
    "    # Fetch the robots.txt file\n",
    "    robots_url = f\"{base_url}/robots.txt\"\n",
    "    response = requests.get(robots_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(f\"robots.txt found at {robots_url}\")\n",
    "        print(\"\\nContent preview:\")\n",
    "        print(response.text[:500], \"...\" if len(response.text) > 500 else \"\")\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"No robots.txt found at {robots_url}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Check robots.txt for our example site\n",
    "robots_content = check_robots_txt(\"https://quotes.toscrape.com/\")\n",
    "\n",
    "\n",
    "# Function to set a proper User-Agent\n",
    "def get_headers():\n",
    "    # It's good practice to identify your scraper\n",
    "    return {\n",
    "        \"User-Agent\": \"PythonScrapingTutorial/1.0 (Educational Purpose)\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0595bd",
   "metadata": {},
   "source": [
    "## HTTP Basics\n",
    "\n",
    "Understanding how HTTP works is essential for effective web scraping. Here are some key concepts:\n",
    "\n",
    "- **HTTP Methods**: GET, POST, PUT, DELETE, etc.\n",
    "- **Status Codes**: 200 (OK), 404 (Not Found), 403 (Forbidden), etc.\n",
    "- **Headers**: Contain metadata about the request/response\n",
    "- **Cookies**: Small pieces of data stored by the browser\n",
    "- **Sessions**: Maintain state between multiple requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "072732b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Headers:\n",
      "Date: Fri, 02 May 2025 20:21:27 GMT\n",
      "Content-Type: text/html; charset=utf-8\n",
      "Transfer-Encoding: chunked\n",
      "Connection: keep-alive\n",
      "Strict-Transport-Security: max-age=0; includeSubDomains; preload\n",
      "Content-Encoding: br\n"
     ]
    }
   ],
   "source": [
    "# Example: Making different types of HTTP requests\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "\n",
    "# GET request (most common for scraping)\n",
    "def make_get_request(url, headers=None, params=None):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params, timeout=10)\n",
    "        response.raise_for_status()  # Raise exception for 4XX/5XX responses\n",
    "        return response\n",
    "    except RequestException as e:\n",
    "        print(f\"Error making GET request: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# POST request (for forms, login pages, etc.)\n",
    "def make_post_request(url, data, headers=None):\n",
    "    try:\n",
    "        response = requests.post(url, data=data, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except RequestException as e:\n",
    "        print(f\"Error making POST request: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Using a session to maintain cookies\n",
    "def use_session(login_url, login_data, protected_url):\n",
    "    with requests.Session() as session:\n",
    "        # Login to the site\n",
    "        try:\n",
    "            login_resp = session.post(login_url, data=login_data)\n",
    "            login_resp.raise_for_status()\n",
    "\n",
    "            # Access protected page with the same session\n",
    "            protected_resp = session.get(protected_url)\n",
    "            protected_resp.raise_for_status()\n",
    "            return protected_resp\n",
    "        except RequestException as e:\n",
    "            print(f\"Session error: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Example of examining response headers\n",
    "sample_url = \"https://quotes.toscrape.com/\"\n",
    "response = make_get_request(sample_url, headers=get_headers())\n",
    "if response:\n",
    "    print(\"Response Headers:\")\n",
    "    for header, value in response.headers.items():\n",
    "        print(f\"{header}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98b417",
   "metadata": {},
   "source": [
    "## Fetch HTML Content\n",
    "\n",
    "Use the requests library to fetch HTML content from a given URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "659c41d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the content from https://quotes.toscrape.com/\n",
      "Response status code: 200\n",
      "\n",
      "HTML Content Preview:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>Quotes to Scrape</title>\n",
      "    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\n",
      "    <link rel=\"stylesheet\" href=\"/static/main.css\">\n",
      "    \n",
      "    \n",
      "</head>\n",
      "<body>\n",
      "    <div class=\"container\">\n",
      "        <div class=\"row header-box\">\n",
      "            <div class=\"col-md-8\">\n",
      "                <h1>\n",
      "                    <a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\n",
      "                </h1>\n",
      "            </div>\n",
      "            <div cla ...\n"
     ]
    }
   ],
   "source": [
    "# Define the URL to scrape\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(f\"Successfully fetched the content from {url}\")\n",
    "    print(f\"Response status code: {response.status_code}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the content. Status code: {response.status_code}\")\n",
    "\n",
    "# View the first 500 characters of the HTML content\n",
    "print(\"\\nHTML Content Preview:\")\n",
    "print(response.text[:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c2853",
   "metadata": {},
   "source": [
    "## Parse HTML with BeautifulSoup\n",
    "\n",
    "Parse the fetched HTML content using BeautifulSoup to create a navigable tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1055b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the webpage: Quotes to Scrape\n",
      "\n",
      "Structure of the HTML:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Quotes to Scrape\n",
      "  </title>\n",
      "  <link href=\"/static/bootstrap.min.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"/static/main.css\" rel=\"stylesheet\"/>\n",
      " </head>\n",
      " <body>\n",
      "  <div class=\"container\">\n",
      "   <div class=\"row header-box\">\n",
      "    <div class=\"col-md-8\">\n",
      "     <h1>\n",
      "      <a href=\"/\" style=\"text-decoration: none\">\n",
      "       Quotes to Scrape\n",
      "      </a>\n",
      "     </h1>\n",
      "    </div>\n",
      "    <div class=\"col-md-4\">\n",
      "     <p>\n",
      "      <a href=\"/login\">\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Print the title of the webpage\n",
    "print(f\"Title of the webpage: {soup.title.text}\")\n",
    "\n",
    "# Print the structure of the HTML in a more readable format\n",
    "print(\"\\nStructure of the HTML:\")\n",
    "print(soup.prettify()[:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77307dc7",
   "metadata": {},
   "source": [
    "## Extract Specific Data\n",
    "\n",
    "Use BeautifulSoup methods to extract specific data such as titles, links, or tables from the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d56a75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text           Author  \\\n",
      "0  “The world as we have created it is a process ...  Albert Einstein   \n",
      "1  “It is our choices, Harry, that show what we t...     J.K. Rowling   \n",
      "2  “There are only two ways to live your life. On...  Albert Einstein   \n",
      "3  “The person, be it gentleman or lady, who has ...      Jane Austen   \n",
      "4  “Imperfection is beauty, madness is genius and...   Marilyn Monroe   \n",
      "\n",
      "                                             Tags  \n",
      "0        [change, deep-thoughts, thinking, world]  \n",
      "1                            [abilities, choices]  \n",
      "2  [inspirational, life, live, miracle, miracles]  \n",
      "3              [aliteracy, books, classic, humor]  \n",
      "4                    [be-yourself, inspirational]  \n",
      "\n",
      "Total number of quotes extracted: 10\n",
      "Number of unique authors: 8\n"
     ]
    }
   ],
   "source": [
    "# Extract all quote elements\n",
    "quotes = soup.find_all(\"div\", class_=\"quote\")\n",
    "\n",
    "# Create empty lists to store data\n",
    "quote_texts = []\n",
    "quote_authors = []\n",
    "quote_tags = []\n",
    "\n",
    "# Extract data from each quote element\n",
    "for quote in quotes:\n",
    "    # Extract the quote text\n",
    "    text = quote.find(\"span\", class_=\"text\").text\n",
    "    quote_texts.append(text)\n",
    "\n",
    "    # Extract the quote author\n",
    "    author = quote.find(\"small\", class_=\"author\").text\n",
    "    quote_authors.append(author)\n",
    "\n",
    "    # Extract the quote tags\n",
    "    tags = quote.find(\"div\", class_=\"tags\")\n",
    "    tag_list = tags.find_all(\"a\", class_=\"tag\")\n",
    "    tags_text = [tag.text for tag in tag_list]\n",
    "    quote_tags.append(tags_text)\n",
    "\n",
    "# Create a pandas DataFrame to organize the data\n",
    "quotes_df = pd.DataFrame(\n",
    "    {\"Text\": quote_texts, \"Author\": quote_authors, \"Tags\": quote_tags}\n",
    ")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(quotes_df.head())\n",
    "\n",
    "# Basic analysis\n",
    "print(f\"\\nTotal number of quotes extracted: {len(quotes_df)}\")\n",
    "print(f\"Number of unique authors: {quotes_df['Author'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd71714",
   "metadata": {},
   "source": [
    "## Handle Pagination\n",
    "\n",
    "Implement logic to handle pagination and scrape data across multiple pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121a9c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://quotes.toscrape.com/page/1/\n",
      "Scraping page 2: https://quotes.toscrape.com/page/2/\n",
      "Scraping page 3: https://quotes.toscrape.com/page/3/\n",
      "\n",
      "Total number of quotes collected: 30\n",
      "\n",
      "First 5 quotes:\n",
      "                                                Text           Author  \\\n",
      "0  “The world as we have created it is a process ...  Albert Einstein   \n",
      "1  “It is our choices, Harry, that show what we t...     J.K. Rowling   \n",
      "2  “There are only two ways to live your life. On...  Albert Einstein   \n",
      "3  “The person, be it gentleman or lady, who has ...      Jane Austen   \n",
      "4  “Imperfection is beauty, madness is genius and...   Marilyn Monroe   \n",
      "\n",
      "                                             Tags  \n",
      "0        [change, deep-thoughts, thinking, world]  \n",
      "1                            [abilities, choices]  \n",
      "2  [inspirational, life, live, miracle, miracles]  \n",
      "3              [aliteracy, books, classic, humor]  \n",
      "4                    [be-yourself, inspirational]  \n",
      "\n",
      "Number of quotes by author:\n",
      "Author\n",
      "Albert Einstein        6\n",
      "J.K. Rowling           3\n",
      "Marilyn Monroe         2\n",
      "Bob Marley             2\n",
      "Dr. Seuss              2\n",
      "Friedrich Nietzsche    1\n",
      "Garrison Keillor       1\n",
      "Mother Teresa          1\n",
      "Ralph Waldo Emerson    1\n",
      "Pablo Neruda           1\n",
      "Allen Saunders         1\n",
      "Mark Twain             1\n",
      "Douglas Adams          1\n",
      "Elie Wiesel            1\n",
      "Steve Martin           1\n",
      "Eleanor Roosevelt      1\n",
      "Thomas A. Edison       1\n",
      "André Gide             1\n",
      "Jane Austen            1\n",
      "Jim Henson             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most common tags:\n",
      "life                  7\n",
      "love                  6\n",
      "inspirational         5\n",
      "humor                 4\n",
      "friends               3\n",
      "books                 2\n",
      "friendship            2\n",
      "lack-of-friendship    1\n",
      "unhappy-marriage      1\n",
      "marriage              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape a single page\n",
    "def scrape_page(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch the content. Status code: {response.status_code}\")\n",
    "        return [], [], []\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract all quote elements\n",
    "    quotes = soup.find_all(\"div\", class_=\"quote\")\n",
    "\n",
    "    # Create empty lists to store data\n",
    "    quote_texts = []\n",
    "    quote_authors = []\n",
    "    quote_tags = []\n",
    "\n",
    "    # Extract data from each quote element\n",
    "    for quote in quotes:\n",
    "        # Extract the quote text\n",
    "        text = quote.find(\"span\", class_=\"text\").text\n",
    "        quote_texts.append(text)\n",
    "\n",
    "        # Extract the quote author\n",
    "        author = quote.find(\"small\", class_=\"author\").text\n",
    "        quote_authors.append(author)\n",
    "\n",
    "        # Extract the quote tags\n",
    "        tags = quote.find(\"div\", class_=\"tags\")\n",
    "        tag_list = tags.find_all(\"a\", class_=\"tag\")\n",
    "        tags_text = [tag.text for tag in tag_list]\n",
    "        quote_tags.append(tags_text)\n",
    "\n",
    "    return quote_texts, quote_authors, quote_tags\n",
    "\n",
    "\n",
    "# Initialize lists to store all data\n",
    "all_texts = []\n",
    "all_authors = []\n",
    "all_tags = []\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://quotes.toscrape.com/page/{}/\"\n",
    "\n",
    "# Number of pages to scrape\n",
    "num_pages = 3\n",
    "\n",
    "# Loop through pages\n",
    "for page in range(1, num_pages + 1):\n",
    "    page_url = base_url.format(page)\n",
    "    print(f\"Scraping page {page}: {page_url}\")\n",
    "\n",
    "    # Scrape the page\n",
    "    texts, authors, tags = scrape_page(page_url)\n",
    "\n",
    "    # Add data to the lists\n",
    "    all_texts.extend(texts)\n",
    "    all_authors.extend(authors)\n",
    "    all_tags.extend(tags)\n",
    "\n",
    "    # Sleep to be respectful to the website\n",
    "    time.sleep(1)\n",
    "\n",
    "# Create a pandas DataFrame with all the data\n",
    "all_quotes_df = pd.DataFrame(\n",
    "    {\"Text\": all_texts, \"Author\": all_authors, \"Tags\": all_tags}\n",
    ")\n",
    "\n",
    "# Display the DataFrame information\n",
    "print(f\"\\nTotal number of quotes collected: {len(all_quotes_df)}\")\n",
    "print(\"\\nFirst 5 quotes:\")\n",
    "print(all_quotes_df.head())\n",
    "\n",
    "# Count quotes by author\n",
    "author_counts = all_quotes_df[\"Author\"].value_counts()\n",
    "print(\"\\nNumber of quotes by author:\")\n",
    "print(author_counts)\n",
    "\n",
    "# Find most common tags\n",
    "# Flatten the list of tags\n",
    "all_tags_flat = [tag for tags_list in all_tags for tag in tags_list]\n",
    "tag_counts = pd.Series(all_tags_flat).value_counts()\n",
    "print(\"\\nMost common tags:\")\n",
    "print(tag_counts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f355b52",
   "metadata": {},
   "source": [
    "## Structured Data: Scraping Tables\n",
    "\n",
    "Many websites present data in structured formats like tables. BeautifulSoup makes it relatively easy to extract this structured data.\n",
    "\n",
    "### HTML Tables\n",
    "\n",
    "HTML tables are defined with the `<table>` tag and contain rows (`<tr>`) and cells (`<td>` or `<th>` for headers). This structure makes them ideal for conversion to pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80553c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table extracted successfully:\n",
      "                        Company          Contact  Country\n",
      "0           Alfreds Futterkiste     Maria Anders  Germany\n",
      "1    Centro comercial Moctezuma  Francisco Chang   Mexico\n",
      "2                  Ernst Handel    Roland Mendel  Austria\n",
      "3                Island Trading    Helen Bennett       UK\n",
      "4  Laughing Bacchus Winecellars  Yoshi Tannamuri   Canada\n",
      "\n",
      "Number of tables found using pd.read_html: 2\n",
      "\n",
      "First table using pd.read_html:\n",
      "                        Company          Contact  Country\n",
      "0           Alfreds Futterkiste     Maria Anders  Germany\n",
      "1    Centro comercial Moctezuma  Francisco Chang   Mexico\n",
      "2                  Ernst Handel    Roland Mendel  Austria\n",
      "3                Island Trading    Helen Bennett       UK\n",
      "4  Laughing Bacchus Winecellars  Yoshi Tannamuri   Canada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zekry\\AppData\\Local\\Temp\\ipykernel_17016\\2602601248.py:52: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(response.text)\n"
     ]
    }
   ],
   "source": [
    "# Example: Scraping a table from a webpage\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of a page with a table (this is a sample with a simple HTML table)\n",
    "table_url = \"https://www.w3schools.com/html/html_tables.asp\"\n",
    "\n",
    "# Fetch the page\n",
    "response = requests.get(table_url, headers=get_headers())\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table (in this case, the first table on the page)\n",
    "    table = soup.find(\"table\")\n",
    "\n",
    "    if table:\n",
    "        # Extract table headers\n",
    "        headers = []\n",
    "        header_row = table.find(\"tr\")\n",
    "        if header_row:\n",
    "            headers = [\n",
    "                header.text.strip() for header in header_row.find_all([\"th\", \"td\"])\n",
    "            ]\n",
    "\n",
    "        # Extract table rows\n",
    "        rows = []\n",
    "        data_rows = table.find_all(\"tr\")[1:] if header_row else table.find_all(\"tr\")\n",
    "\n",
    "        for row in data_rows:\n",
    "            cells = row.find_all([\"td\", \"th\"])\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            rows.append(row_data)\n",
    "\n",
    "        # Create pandas DataFrame\n",
    "        if headers:\n",
    "            df = pd.DataFrame(rows, columns=headers)\n",
    "        else:\n",
    "            df = pd.DataFrame(rows)\n",
    "\n",
    "        print(\"Table extracted successfully:\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"No table found on the page\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "\n",
    "# Alternative: Using pandas' built-in read_html function\n",
    "# This function automatically extracts tables from HTML\n",
    "try:\n",
    "    tables = pd.read_html(response.text)\n",
    "    print(f\"\\nNumber of tables found using pd.read_html: {len(tables)}\")\n",
    "    print(\"\\nFirst table using pd.read_html:\")\n",
    "    print(tables[0].head())\n",
    "except Exception as e:\n",
    "    print(f\"Error using pd.read_html: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d391c3",
   "metadata": {},
   "source": [
    "### Exercise: Scraping Structured Data\n",
    "\n",
    "Try to scrape a table from a website of your choice. Here are some ideas:\n",
    "\n",
    "1. World population data\n",
    "2. Stock market data\n",
    "3. Sports statistics\n",
    "4. Weather data\n",
    "\n",
    "Remember to check the website's robots.txt first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3cb8424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robots.txt found at https://en.wikipedia.org/robots.txt\n",
      "\n",
      "Content preview:\n",
      "﻿# robots.txt for http://www.wikipedia.org/ and friends\n",
      "#\n",
      "# Please note: There are a lot of pages on this site, and there are\n",
      "# some misbehaved spiders out there that go _way_ too fast. If you're\n",
      "# irresponsible, your access to the site may be blocked.\n",
      "#\n",
      "\n",
      "# Observed spamming large amounts of https://en.wikipedia.org/?curid=NNNNNN\n",
      "# and ignoring 429 ratelimit responses, claims to respect robots:\n",
      "# http://mj12bot.com/\n",
      "User-agent: MJ12bot\n",
      "Disallow: /\n",
      "\n",
      "# advertising-related bots:\n",
      "User-agent: Mediapa ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ufeff# robots.txt for http://www.wikipedia.org/ and friends\\n#\\n# Please note: There are a lot of pages on this site, and there are\\n# some misbehaved spiders out there that go _way_ too fast. If you\\'re\\n# irresponsible, your access to the site may be blocked.\\n#\\n\\n# Observed spamming large amounts of https://en.wikipedia.org/?curid=NNNNNN\\n# and ignoring 429 ratelimit responses, claims to respect robots:\\n# http://mj12bot.com/\\nUser-agent: MJ12bot\\nDisallow: /\\n\\n# advertising-related bots:\\nUser-agent: Mediapartners-Google*\\nDisallow: /\\n\\n# Wikipedia work bots:\\nUser-agent: IsraBot\\nDisallow:\\n\\nUser-agent: Orthogaffe\\nDisallow:\\n\\n# Crawlers that are kind enough to obey, but which we\\'d rather not have\\n# unless they\\'re feeding search engines.\\nUser-agent: UbiCrawler\\nDisallow: /\\n\\nUser-agent: DOC\\nDisallow: /\\n\\nUser-agent: Zao\\nDisallow: /\\n\\n# Some bots are known to be trouble, particularly those designed to copy\\n# entire sites. Please obey robots.txt.\\nUser-agent: sitecheck.internetseer.com\\nDisallow: /\\n\\nUser-agent: Zealbot\\nDisallow: /\\n\\nUser-agent: MSIECrawler\\nDisallow: /\\n\\nUser-agent: SiteSnagger\\nDisallow: /\\n\\nUser-agent: WebStripper\\nDisallow: /\\n\\nUser-agent: WebCopier\\nDisallow: /\\n\\nUser-agent: Fetch\\nDisallow: /\\n\\nUser-agent: Offline Explorer\\nDisallow: /\\n\\nUser-agent: Teleport\\nDisallow: /\\n\\nUser-agent: TeleportPro\\nDisallow: /\\n\\nUser-agent: WebZIP\\nDisallow: /\\n\\nUser-agent: linko\\nDisallow: /\\n\\nUser-agent: HTTrack\\nDisallow: /\\n\\nUser-agent: Microsoft.URL.Control\\nDisallow: /\\n\\nUser-agent: Xenu\\nDisallow: /\\n\\nUser-agent: larbin\\nDisallow: /\\n\\nUser-agent: libwww\\nDisallow: /\\n\\nUser-agent: ZyBORG\\nDisallow: /\\n\\nUser-agent: Download Ninja\\nDisallow: /\\n\\n# Misbehaving: requests much too fast:\\nUser-agent: fast\\nDisallow: /\\n\\n#\\n# Sorry, wget in its recursive mode is a frequent problem.\\n# Please read the man page and use it properly; there is a\\n# --wait option you can use to set the delay between hits,\\n# for instance.\\n#\\nUser-agent: wget\\nDisallow: /\\n\\n#\\n# The \\'grub\\' distributed client has been *very* poorly behaved.\\n#\\nUser-agent: grub-client\\nDisallow: /\\n\\n#\\n# Doesn\\'t follow robots.txt anyway, but...\\n#\\nUser-agent: k2spider\\nDisallow: /\\n\\n#\\n# Hits many times per second, not acceptable\\n# http://www.nameprotect.com/botinfo.html\\nUser-agent: NPBot\\nDisallow: /\\n\\n# A capture bot, downloads gazillions of pages with no public benefit\\n# http://www.webreaper.net/\\nUser-agent: WebReaper\\nDisallow: /\\n\\n\\n#\\n# Friendly, low-speed bots are welcome viewing article pages, but not\\n# dynamically-generated pages please.\\n#\\n# Inktomi\\'s \"Slurp\" can read a minimum delay between hits; if your\\n# bot supports such a thing using the \\'Crawl-delay\\' or another\\n# instruction, please let us know.\\n#\\n# There is a special exception for API mobileview to allow dynamic\\n# mobile web & app views to load section content.\\n# These views aren\\'t HTTP-cached but use parser cache aggressively\\n# and don\\'t expose special: pages etc.\\n#\\n# Another exception is for REST API documentation, located at\\n# /api/rest_v1/?doc.\\n#\\nUser-agent: *\\nAllow: /w/api.php?action=mobileview&\\nAllow: /w/load.php?\\nAllow: /api/rest_v1/?doc\\nDisallow: /w/\\nDisallow: /api/\\nDisallow: /trap/\\nDisallow: /wiki/Special:\\nDisallow: /wiki/Spezial:\\nDisallow: /wiki/Spesial:\\nDisallow: /wiki/Special%3A\\nDisallow: /wiki/Spezial%3A\\nDisallow: /wiki/Spesial%3A\\n#\\n# ar:\\nDisallow: /wiki/%D8%AE%D8%A7%D8%B5:Search\\nDisallow: /wiki/%D8%AE%D8%A7%D8%B5%3ASearch\\n#\\n# dewiki:\\n# T6937\\n# sensible deletion and meta user discussion pages:\\nDisallow: /wiki/Wikipedia:L%C3%B6schkandidaten/\\nDisallow: /wiki/Wikipedia:Löschkandidaten/\\nDisallow: /wiki/Wikipedia:Vandalensperrung/\\nDisallow: /wiki/Wikipedia:Benutzersperrung/\\nDisallow: /wiki/Wikipedia:Vermittlungsausschuss/\\nDisallow: /wiki/Wikipedia:Administratoren/Probleme/\\nDisallow: /wiki/Wikipedia:Adminkandidaturen/\\nDisallow: /wiki/Wikipedia:Qualitätssicherung/\\nDisallow: /wiki/Wikipedia:Qualit%C3%A4tssicherung/\\n# 4937#5\\nDisallow: /wiki/Wikipedia:Vandalismusmeldung/\\nDisallow: /wiki/Wikipedia:Gesperrte_Lemmata/\\nDisallow: /wiki/Wikipedia:Löschprüfung/\\nDisallow: /wiki/Wikipedia:L%C3%B6schprüfung/\\nDisallow: /wiki/Wikipedia:Administratoren/Notizen/\\nDisallow: /wiki/Wikipedia:Schiedsgericht/Anfragen/\\nDisallow: /wiki/Wikipedia:L%C3%B6schpr%C3%BCfung/\\n# T14111\\nDisallow: /wiki/Wikipedia:Checkuser/\\nDisallow: /wiki/Wikipedia_Diskussion:Checkuser/\\nDisallow: /wiki/Wikipedia_Diskussion:Adminkandidaturen/\\n# T15961\\nDisallow: /wiki/Wikipedia:Spam-Blacklist-Log\\nDisallow: /wiki/Wikipedia%3ASpam-Blacklist-Log\\nDisallow: /wiki/Wikipedia_Diskussion:Spam-Blacklist-Log\\nDisallow: /wiki/Wikipedia_Diskussion%3ASpam-Blacklist-Log\\n#\\n# enwiki:\\n# Folks get annoyed when VfD discussions end up the number 1 google hit for\\n# their name. See T6776\\nDisallow: /wiki/Wikipedia:Articles_for_deletion/\\nDisallow: /wiki/Wikipedia%3AArticles_for_deletion/\\nDisallow: /wiki/Wikipedia:Votes_for_deletion/\\nDisallow: /wiki/Wikipedia%3AVotes_for_deletion/\\nDisallow: /wiki/Wikipedia:Pages_for_deletion/\\nDisallow: /wiki/Wikipedia%3APages_for_deletion/\\nDisallow: /wiki/Wikipedia:Miscellany_for_deletion/\\nDisallow: /wiki/Wikipedia%3AMiscellany_for_deletion/\\nDisallow: /wiki/Wikipedia:Miscellaneous_deletion/\\nDisallow: /wiki/Wikipedia%3AMiscellaneous_deletion/\\nDisallow: /wiki/Wikipedia:Copyright_problems\\nDisallow: /wiki/Wikipedia%3ACopyright_problems\\nDisallow: /wiki/Wikipedia:Protected_titles/\\nDisallow: /wiki/Wikipedia%3AProtected_titles/\\n# T15398\\nDisallow: /wiki/Wikipedia:WikiProject_Spam/\\nDisallow: /wiki/Wikipedia%3AWikiProject_Spam/\\n# T16075\\nDisallow: /wiki/MediaWiki:Spam-blacklist\\nDisallow: /wiki/MediaWiki%3ASpam-blacklist\\nDisallow: /wiki/MediaWiki_talk:Spam-blacklist\\nDisallow: /wiki/MediaWiki_talk%3ASpam-blacklist\\n# T13261\\nDisallow: /wiki/Wikipedia:Requests_for_arbitration/\\nDisallow: /wiki/Wikipedia%3ARequests_for_arbitration/\\nDisallow: /wiki/Wikipedia:Requests_for_comment/\\nDisallow: /wiki/Wikipedia%3ARequests_for_comment/\\nDisallow: /wiki/Wikipedia:Requests_for_adminship/\\nDisallow: /wiki/Wikipedia%3ARequests_for_adminship/\\n# T12288\\nDisallow: /wiki/Wikipedia_talk:Articles_for_deletion/\\nDisallow: /wiki/Wikipedia_talk%3AArticles_for_deletion/\\nDisallow: /wiki/Wikipedia_talk:Votes_for_deletion/\\nDisallow: /wiki/Wikipedia_talk%3AVotes_for_deletion/\\nDisallow: /wiki/Wikipedia_talk:Pages_for_deletion/\\nDisallow: /wiki/Wikipedia_talk%3APages_for_deletion/\\nDisallow: /wiki/Wikipedia_talk:Miscellany_for_deletion/\\nDisallow: /wiki/Wikipedia_talk%3AMiscellany_for_deletion/\\nDisallow: /wiki/Wikipedia_talk:Miscellaneous_deletion/\\nDisallow: /wiki/Wikipedia_talk%3AMiscellaneous_deletion/\\n# T16793\\nDisallow: /wiki/Wikipedia:Changing_username\\nDisallow: /wiki/Wikipedia%3AChanging_username\\nDisallow: /wiki/Wikipedia:Changing_username/\\nDisallow: /wiki/Wikipedia%3AChanging_username/\\nDisallow: /wiki/Wikipedia_talk:Changing_username\\nDisallow: /wiki/Wikipedia_talk%3AChanging_username\\nDisallow: /wiki/Wikipedia_talk:Changing_username/\\nDisallow: /wiki/Wikipedia_talk%3AChanging_username/\\n#\\n# eswiki:\\n# T8746\\nDisallow: /wiki/Wikipedia:Consultas_de_borrado/\\nDisallow: /wiki/Wikipedia%3AConsultas_de_borrado/\\n#\\n# fiwiki:\\n# T10695\\nDisallow: /wiki/Wikipedia:Poistettavat_sivut\\nDisallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\\nDisallow: /wiki/Käyttäjä:\\nDisallow: /wiki/Keskustelu_k%C3%A4ytt%C3%A4j%C3%A4st%C3%A4:\\nDisallow: /wiki/Keskustelu_käyttäjästä:\\nDisallow: /wiki/Wikipedia:Yll%C3%A4pit%C3%A4j%C3%A4t/\\nDisallow: /wiki/Wikipedia:Ylläpitäjät/\\n#\\n# hewiki:\\nDisallow: /wiki/%D7%9E%D7%99%D7%95%D7%97%D7%93:Search\\nDisallow: /wiki/%D7%9E%D7%99%D7%95%D7%97%D7%93%3ASearch\\n#T11517\\nDisallow: /wiki/ויקיפדיה:רשימת_מועמדים_למחיקה/\\nDisallow: /wiki/ויקיפדיה%3Aרשימת_מועמדים_למחיקה/\\nDisallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94:%D7%A8%D7%A9%D7%99%D7%9E%D7%AA_%D7%9E%D7%95%D7%A2%D7%9E%D7%93%D7%99%D7%9D_%D7%9C%D7%9E%D7%97%D7%99%D7%A7%D7%94/\\nDisallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94%3A%D7%A8%D7%A9%D7%99%D7%9E%D7%AA_%D7%9E%D7%95%D7%A2%D7%9E%D7%93%D7%99%D7%9D_%D7%9C%D7%9E%D7%97%D7%99%D7%A7%D7%94/\\nDisallow: /wiki/ויקיפדיה:ערכים_לא_קיימים_ומוגנים\\nDisallow: /wiki/ויקיפדיה%3Aערכים_לא_קיימים_ומוגנים\\nDisallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94:%D7%A2%D7%A8%D7%9B%D7%99%D7%9D_%D7%9C%D7%90_%D7%A7%D7%99%D7%99%D7%9E%D7%99%D7%9D_%D7%95%D7%9E%D7%95%D7%92%D7%A0%D7%99%D7%9D\\nDisallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94%3A%D7%A2%D7%A8%D7%9B%D7%99%D7%9D_%D7%9C%D7%90_%D7%A7%D7%99%D7%99%D7%9E%D7%99%D7%9D_%D7%95%D7%9E%D7%95%D7%92%D7%A0%D7%99%D7%9D\\nDisallow: /wiki/ויקיפדיה:דפים_לא_קיימים_ומוגנים\\nDisallow: /wiki/ויקיפדיה%3Aדפים_לא_קיימים_ומוגנים\\nDisallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94:%D7%93%D7%A4%D7%99%D7%9D_%D7%9C%D7%90_%D7%A7%D7%99%D7%99%D7%9E%D7%99%D7%9D_%D7%95%D7%9E%D7%95%D7%92%D7%A0%D7%99%D7%9D\\nDisallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94%3A%D7%93%D7%A4%D7%99%D7%9D_%D7%9C%D7%90_%D7%A7%D7%99%D7%99%D7%9E%D7%99%D7%9D_%D7%95%D7%9E%D7%95%D7%92%D7%A0%D7%99%D7%9D\\n#\\n# huwiki:\\nDisallow: /wiki/Speci%C3%A1lis:Search\\nDisallow: /wiki/Speci%C3%A1lis%3ASearch\\n#\\n# itwiki:\\n# T7545\\nDisallow: /wiki/Wikipedia:Pagine_da_cancellare\\nDisallow: /wiki/Wikipedia%3APagine_da_cancellare\\nDisallow: /wiki/Wikipedia:Utenti_problematici\\nDisallow: /wiki/Wikipedia%3AUtenti_problematici\\nDisallow: /wiki/Wikipedia:Vandalismi_in_corso\\nDisallow: /wiki/Wikipedia%3AVandalismi_in_corso\\nDisallow: /wiki/Wikipedia:Amministratori\\nDisallow: /wiki/Wikipedia%3AAmministratori\\nDisallow: /wiki/Wikipedia:Proposte_di_cancellazione_semplificata\\nDisallow: /wiki/Wikipedia%3AProposte_di_cancellazione_semplificata\\nDisallow: /wiki/Categoria:Da_cancellare_subito\\nDisallow: /wiki/Categoria%3ADa_cancellare_subito\\nDisallow: /wiki/Wikipedia:Sospette_violazioni_di_copyright\\nDisallow: /wiki/Wikipedia%3ASospette_violazioni_di_copyright\\nDisallow: /wiki/Categoria:Da_controllare_per_copyright\\nDisallow: /wiki/Categoria%3ADa_controllare_per_copyright\\nDisallow: /wiki/Progetto:Rimozione_contributi_sospetti\\nDisallow: /wiki/Progetto%3ARimozione_contributi_sospetti\\nDisallow: /wiki/Categoria:Da_cancellare_subito_per_violazione_integrale_copyright\\nDisallow: /wiki/Categoria%3ADa_cancellare_subito_per_violazione_integrale_copyright\\nDisallow: /wiki/Progetto:Cococo\\nDisallow: /wiki/Progetto%3ACococo\\nDisallow: /wiki/Discussioni_progetto:Cococo\\nDisallow: /wiki/Discussioni_progetto%3ACococo\\n#\\n# jawiki\\nDisallow: /wiki/%E7%89%B9%E5%88%A5:Search\\nDisallow: /wiki/%E7%89%B9%E5%88%A5%3ASearch\\n# T7239\\nDisallow: /wiki/Wikipedia:%E5%89%8A%E9%99%A4%E4%BE%9D%E9%A0%BC/\\nDisallow: /wiki/Wikipedia%3A%E5%89%8A%E9%99%A4%E4%BE%9D%E9%A0%BC/\\nDisallow: /wiki/Wikipedia:%E5%88%A9%E7%94%A8%E8%80%85%E3%83%9A%E3%83%BC%E3%82%B8%E3%81%AE%E5%89%8A%E9%99%A4%E4%BE%9D%E9%A0%BC\\nDisallow: /wiki/Wikipedia%3A%E5%88%A9%E7%94%A8%E8%80%85%E3%83%9A%E3%83%BC%E3%82%B8%E3%81%AE%E5%89%8A%E9%99%A4%E4%BE%9D%E9%A0%BC\\n# nowiki\\n# T13432\\nDisallow: /wiki/Bruker:\\nDisallow: /wiki/Bruker%3A\\nDisallow: /wiki/Brukerdiskusjon\\nDisallow: /wiki/Wikipedia:Administratorer\\nDisallow: /wiki/Wikipedia%3AAdministratorer\\nDisallow: /wiki/Wikipedia-diskusjon:Administratorer\\nDisallow: /wiki/Wikipedia-diskusjon%3AAdministratorer\\nDisallow: /wiki/Wikipedia:Sletting\\nDisallow: /wiki/Wikipedia%3ASletting\\nDisallow: /wiki/Wikipedia-diskusjon:Sletting\\nDisallow: /wiki/Wikipedia-diskusjon%3ASletting\\n#\\n# plwiki\\n# T10067\\nDisallow: /wiki/Wikipedia:Strony_do_usuni%C4%99cia\\nDisallow: /wiki/Wikipedia%3AStrony_do_usuni%C4%99cia\\nDisallow: /wiki/Wikipedia:Do_usuni%C4%99cia\\nDisallow: /wiki/Wikipedia%3ADo_usuni%C4%99cia\\nDisallow: /wiki/Wikipedia:SDU/\\nDisallow: /wiki/Wikipedia%3ASDU/\\nDisallow: /wiki/Wikipedia:Strony_podejrzane_o_naruszenie_praw_autorskich\\nDisallow: /wiki/Wikipedia%3AStrony_podejrzane_o_naruszenie_praw_autorskich\\n#\\n# ptwiki:\\n# T7394\\nDisallow: /wiki/Wikipedia:Páginas_para_eliminar/\\nDisallow: /wiki/Wikipedia:P%C3%A1ginas_para_eliminar/\\nDisallow: /wiki/Wikipedia%3AP%C3%A1ginas_para_eliminar/\\nDisallow: /wiki/Wikipedia_Discussão:Páginas_para_eliminar/\\nDisallow: /wiki/Wikipedia_Discuss%C3%A3o:P%C3%A1ginas_para_eliminar/\\nDisallow: /wiki/Wikipedia_Discuss%C3%A3o%3AP%C3%A1ginas_para_eliminar/\\n#\\n# rowiki:\\n# T14546\\nDisallow: /wiki/Wikipedia:Pagini_de_%C5%9Fters\\nDisallow: /wiki/Wikipedia%3APagini_de_%C5%9Fters\\nDisallow: /wiki/Discu%C5%A3ie_Wikipedia:Pagini_de_%C5%9Fters\\nDisallow: /wiki/Discu%C5%A3ie_Wikipedia%3APagini_de_%C5%9Fters\\n#\\n# ruwiki:\\nDisallow: /wiki/%D0%A1%D0%BF%D0%B5%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5:Search\\nDisallow: /wiki/%D0%A1%D0%BF%D0%B5%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%3ASearch\\n#\\n# svwiki:\\n# T12229\\nDisallow: /wiki/Wikipedia%3ASidor_f%C3%B6reslagna_f%C3%B6r_radering\\nDisallow: /wiki/Wikipedia:Sidor_f%C3%B6reslagna_f%C3%B6r_radering\\nDisallow: /wiki/Wikipedia:Sidor_föreslagna_för_radering\\nDisallow: /wiki/Användare\\nDisallow: /wiki/Anv%C3%A4ndare\\nDisallow: /wiki/Användardiskussion\\nDisallow: /wiki/Anv%C3%A4ndardiskussion\\nDisallow: /wiki/Wikipedia:Skyddade_sidnamn\\nDisallow: /wiki/Wikipedia%3ASkyddade_sidnamn\\n# T13291\\nDisallow: /wiki/Wikipedia:Sidor_som_bör_raderas\\nDisallow: /wiki/Wikipedia:Sidor_som_b%C3%B6r_raderas\\nDisallow: /wiki/Wikipedia%3ASidor_som_b%C3%B6r_raderas\\n#\\n# zhwiki:\\n# T7104\\nDisallow: /wiki/Wikipedia:删除投票/侵权\\nDisallow: /wiki/Wikipedia:%E5%88%A0%E9%99%A4%E6%8A%95%E7%A5%A8/%E4%BE%B5%E6%9D%83\\nDisallow: /wiki/Wikipedia:删除投票和请求\\nDisallow: /wiki/Wikipedia:%E5%88%A0%E9%99%A4%E6%8A%95%E7%A5%A8%E5%92%8C%E8%AF%B7%E6%B1%82\\nDisallow: /wiki/Category:快速删除候选\\nDisallow: /wiki/Category:%E5%BF%AB%E9%80%9F%E5%88%A0%E9%99%A4%E5%80%99%E9%80%89\\nDisallow: /wiki/Category:维基百科需要翻译的文章\\nDisallow: /wiki/Category:%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E9%9C%80%E8%A6%81%E7%BF%BB%E8%AF%91%E7%9A%84%E6%96%87%E7%AB%A0\\n#\\n# sister projects\\n#\\n# enwikinews:\\n# T7340\\nDisallow: /wiki/Portal:Prepared_stories/\\nDisallow: /wiki/Portal%3APrepared_stories/\\n#\\n# itwikinews\\n# T11138\\nDisallow: /wiki/Wikinotizie:Richieste_di_cancellazione\\nDisallow: /wiki/Wikinotizie:Sospette_violazioni_di_copyright\\nDisallow: /wiki/Categoria:Da_cancellare_subito\\nDisallow: /wiki/Categoria:Da_cancellare_subito_per_violazione_integrale_copyright\\nDisallow: /wiki/Wikinotizie:Storie_in_preparazione\\n#\\n# enwikiquote:\\n# T17095\\nDisallow: /wiki/Wikiquote:Votes_for_deletion/\\nDisallow: /wiki/Wikiquote%3AVotes_for_deletion/\\nDisallow: /wiki/Wikiquote_talk:Votes_for_deletion/\\nDisallow: /wiki/Wikiquote_talk%3AVotes_for_deletion/\\nDisallow: /wiki/Wikiquote:Votes_for_deletion_archive/\\nDisallow: /wiki/Wikiquote%3AVotes_for_deletion_archive/\\nDisallow: /wiki/Wikiquote_talk:Votes_for_deletion_archive/\\nDisallow: /wiki/Wikiquote_talk%3AVotes_for_deletion_archive/\\n#\\n# enwikibooks\\nDisallow: /wiki/Wikibooks:Votes_for_deletion\\n#\\n# working...\\nDisallow: /wiki/Fundraising_2007/comments\\n#\\n#\\n#\\n#----------------------------------------------------------#\\n#\\n#\\n#\\n#\\n# Localisable part of robots.txt for en.wikipedia.org\\n#\\n# Edit at https://en.wikipedia.org/w/index.php?title=MediaWiki:Robots.txt&action=edit\\n# Don\\'t add newlines here. All rules set here are active for every user-agent.\\n#\\n# Please check any changes using a syntax validator\\n# Enter https://en.wikipedia.org/robots.txt as the URL to check.\\n#\\n# https://phabricator.wikimedia.org/T16075\\nDisallow: /wiki/MediaWiki:Spam-blacklist\\nDisallow: /wiki/MediaWiki%3ASpam-blacklist\\nDisallow: /wiki/MediaWiki_talk:Spam-blacklist\\nDisallow: /wiki/MediaWiki_talk%3ASpam-blacklist\\nDisallow: /wiki/Wikipedia:WikiProject_Spam\\nDisallow: /wiki/Wikipedia_talk:WikiProject_Spam\\n#\\n# Folks get annoyed when XfD discussions end up the number 1 google hit for\\n# their name. \\n# https://phabricator.wikimedia.org/T16075\\nDisallow: /wiki/Wikipedia:Articles_for_deletion\\nDisallow: /wiki/Wikipedia%3AArticles_for_deletion\\nDisallow: /wiki/Wikipedia:Votes_for_deletion\\nDisallow: /wiki/Wikipedia%3AVotes_for_deletion\\nDisallow: /wiki/Wikipedia:Pages_for_deletion\\nDisallow: /wiki/Wikipedia%3APages_for_deletion\\nDisallow: /wiki/Wikipedia:Miscellany_for_deletion\\nDisallow: /wiki/Wikipedia%3AMiscellany_for_deletion\\nDisallow: /wiki/Wikipedia:Miscellaneous_deletion\\nDisallow: /wiki/Wikipedia%3AMiscellaneous_deletion\\nDisallow: /wiki/Wikipedia:Categories_for_discussion\\nDisallow: /wiki/Wikipedia%3ACategories_for_discussion\\nDisallow: /wiki/Wikipedia:Templates_for_deletion\\nDisallow: /wiki/Wikipedia%3ATemplates_for_deletion\\nDisallow: /wiki/Wikipedia:Redirects_for_discussion\\nDisallow: /wiki/Wikipedia%3ARedirects_for_discussion\\nDisallow: /wiki/Wikipedia:Deletion_review\\nDisallow: /wiki/Wikipedia%3ADeletion_review\\nDisallow: /wiki/Wikipedia:WikiProject_Deletion_sorting\\nDisallow: /wiki/Wikipedia%3AWikiProject_Deletion_sorting\\nDisallow: /wiki/Wikipedia:Files_for_deletion\\nDisallow: /wiki/Wikipedia%3AFiles_for_deletion\\nDisallow: /wiki/Wikipedia:Files_for_discussion\\nDisallow: /wiki/Wikipedia%3AFiles_for_discussion\\nDisallow: /wiki/Wikipedia:Possibly_unfree_files\\nDisallow: /wiki/Wikipedia%3APossibly_unfree_files\\n#\\n# https://phabricator.wikimedia.org/T12288\\nDisallow: /wiki/Wikipedia_talk:Articles_for_deletion\\nDisallow: /wiki/Wikipedia_talk%3AArticles_for_deletion\\nDisallow: /wiki/Wikipedia_talk:Votes_for_deletion\\nDisallow: /wiki/Wikipedia_talk%3AVotes_for_deletion\\nDisallow: /wiki/Wikipedia_talk:Pages_for_deletion\\nDisallow: /wiki/Wikipedia_talk%3APages_for_deletion\\nDisallow: /wiki/Wikipedia_talk:Miscellany_for_deletion\\nDisallow: /wiki/Wikipedia_talk%3AMiscellany_for_deletion\\nDisallow: /wiki/Wikipedia_talk:Miscellaneous_deletion\\nDisallow: /wiki/Wikipedia_talk%3AMiscellaneous_deletion\\nDisallow: /wiki/Wikipedia_talk:Templates_for_deletion\\nDisallow: /wiki/Wikipedia_talk%3ATemplates_for_deletion\\nDisallow: /wiki/Wikipedia_talk:Categories_for_discussion\\nDisallow: /wiki/Wikipedia_talk%3ACategories_for_discussion\\nDisallow: /wiki/Wikipedia_talk:Deletion_review\\nDisallow: /wiki/Wikipedia_talk%3ADeletion_review\\nDisallow: /wiki/Wikipedia_talk:WikiProject_Deletion_sorting\\nDisallow: /wiki/Wikipedia_talk%3AWikiProject_Deletion_sorting\\nDisallow: /wiki/Wikipedia_talk:Files_for_deletion\\nDisallow: /wiki/Wikipedia_talk%3AFiles_for_deletion\\nDisallow: /wiki/Wikipedia_talk:Files_for_discussion\\nDisallow: /wiki/Wikipedia_talk%3AFiles_for_discussion\\nDisallow: /wiki/Wikipedia_talk:Possibly_unfree_files\\nDisallow: /wiki/Wikipedia_talk%3APossibly_unfree_files\\n#\\nDisallow: /wiki/Wikipedia:Copyright_problems\\nDisallow: /wiki/Wikipedia%3ACopyright_problems\\nDisallow: /wiki/Wikipedia_talk:Copyright_problems\\nDisallow: /wiki/Wikipedia_talk%3ACopyright_problems\\nDisallow: /wiki/Wikipedia:Suspected_copyright_violations\\nDisallow: /wiki/Wikipedia%3ASuspected_copyright_violations\\nDisallow: /wiki/Wikipedia_talk:Suspected_copyright_violations\\nDisallow: /wiki/Wikipedia_talk%3ASuspected_copyright_violations\\nDisallow: /wiki/Wikipedia:Contributor_copyright_investigations\\nDisallow: /wiki/Wikipedia%3AContributor_copyright_investigations\\nDisallow: /wiki/Wikipedia:Contributor_copyright_investigations\\nDisallow: /wiki/Wikipedia%3AContributor_copyright_investigations\\nDisallow: /wiki/Wikipedia_talk:Contributor_copyright_investigations\\nDisallow: /wiki/Wikipedia_talk%3AContributor_copyright_investigations\\nDisallow: /wiki/Wikipedia_talk:Contributor_copyright_investigations\\nDisallow: /wiki/Wikipedia_talk%3AContributor_copyright_investigations\\nDisallow: /wiki/Wikipedia:Protected_titles\\nDisallow: /wiki/Wikipedia%3AProtected_titles\\nDisallow: /wiki/Wikipedia_talk:Protected_titles\\nDisallow: /wiki/Wikipedia_talk%3AProtected_titles\\nDisallow: /wiki/Wikipedia:Articles_for_creation\\nDisallow: /wiki/Wikipedia%3AArticles_for_creation\\nDisallow: /wiki/Wikipedia_talk:Articles_for_creation\\nDisallow: /wiki/Wikipedia_talk%3AArticles_for_creation\\nDisallow: /wiki/Wikipedia_talk:Article_wizard\\nDisallow: /wiki/Wikipedia_talk%3AArticle_wizard\\n#\\n# https://phabricator.wikimedia.org/T13261\\nDisallow: /wiki/Wikipedia:Requests_for_arbitration\\nDisallow: /wiki/Wikipedia%3ARequests_for_arbitration\\nDisallow: /wiki/Wikipedia_talk:Requests_for_arbitration\\nDisallow: /wiki/Wikipedia_talk%3ARequests_for_arbitration\\nDisallow: /wiki/Wikipedia:Requests_for_comment\\nDisallow: /wiki/Wikipedia%3ARequests_for_comment\\nDisallow: /wiki/Wikipedia_talk:Requests_for_comment\\nDisallow: /wiki/Wikipedia_talk%3ARequests_for_comment\\nDisallow: /wiki/Wikipedia:Requests_for_adminship\\nDisallow: /wiki/Wikipedia%3ARequests_for_adminship\\nDisallow: /wiki/Wikipedia_talk:Requests_for_adminship\\nDisallow: /wiki/Wikipedia_talk%3ARequests_for_adminship\\n#\\n# https://phabricator.wikimedia.org/T14111\\nDisallow: /wiki/Wikipedia:Requests_for_checkuser\\nDisallow: /wiki/Wikipedia%3ARequests_for_checkuser\\nDisallow: /wiki/Wikipedia_talk:Requests_for_checkuser\\nDisallow: /wiki/Wikipedia_talk%3ARequests_for_checkuser\\n#\\n# https://phabricator.wikimedia.org/T15398\\nDisallow: /wiki/Wikipedia:WikiProject_Spam\\nDisallow: /wiki/Wikipedia%3AWikiProject_Spam\\n#\\n# https://phabricator.wikimedia.org/T16793\\nDisallow: /wiki/Wikipedia:Changing_username\\nDisallow: /wiki/Wikipedia%3AChanging_username\\nDisallow: /wiki/Wikipedia:Changing_username\\nDisallow: /wiki/Wikipedia%3AChanging_username\\nDisallow: /wiki/Wikipedia_talk:Changing_username\\nDisallow: /wiki/Wikipedia_talk%3AChanging_username\\nDisallow: /wiki/Wikipedia_talk:Changing_username\\nDisallow: /wiki/Wikipedia_talk%3AChanging_username\\n#\\nDisallow: /wiki/Wikipedia:Administrators%27_noticeboard\\nDisallow: /wiki/Wikipedia%3AAdministrators%27_noticeboard\\nDisallow: /wiki/Wikipedia_talk:Administrators%27_noticeboard\\nDisallow: /wiki/Wikipedia_talk%3AAdministrators%27_noticeboard\\nDisallow: /wiki/Wikipedia:Community_sanction_noticeboard\\nDisallow: /wiki/Wikipedia%3ACommunity_sanction_noticeboard\\nDisallow: /wiki/Wikipedia_talk:Community_sanction_noticeboard\\nDisallow: /wiki/Wikipedia_talk%3ACommunity_sanction_noticeboard\\nDisallow: /wiki/Wikipedia:Bureaucrats%27_noticeboard\\nDisallow: /wiki/Wikipedia%3ABureaucrats%27_noticeboard\\nDisallow: /wiki/Wikipedia_talk:Bureaucrats%27_noticeboard\\nDisallow: /wiki/Wikipedia_talk%3ABureaucrats%27_noticeboard\\n#\\nDisallow: /wiki/Wikipedia:Sockpuppet_investigations\\nDisallow: /wiki/Wikipedia%3ASockpuppet_investigations\\nDisallow: /wiki/Wikipedia_talk:Sockpuppet_investigations\\nDisallow: /wiki/Wikipedia_talk%3ASockpuppet_investigations\\n#\\nDisallow: /wiki/Wikipedia:Neutral_point_of_view/Noticeboard\\nDisallow: /wiki/Wikipedia%3ANeutral_point_of_view/Noticeboard\\nDisallow: /wiki/Wikipedia_talk:Neutral_point_of_view/Noticeboard\\nDisallow: /wiki/Wikipedia_talk%3ANeutral_point_of_view/Noticeboard\\n#\\nDisallow: /wiki/Wikipedia:No_original_research/noticeboard\\nDisallow: /wiki/Wikipedia%3ANo_original_research/noticeboard\\nDisallow: /wiki/Wikipedia_talk:No_original_research/noticeboard\\nDisallow: /wiki/Wikipedia_talk%3ANo_original_research/noticeboard\\n#\\nDisallow: /wiki/Wikipedia:Fringe_theories/Noticeboard\\nDisallow: /wiki/Wikipedia%3AFringe_theories/Noticeboard\\nDisallow: /wiki/Wikipedia_talk:Fringe_theories/Noticeboard\\nDisallow: /wiki/Wikipedia_talk%3AFringe_theories/Noticeboard\\n#\\nDisallow: /wiki/Wikipedia:Conflict_of_interest/Noticeboard\\nDisallow: /wiki/Wikipedia%3AConflict_of_interest/Noticeboard\\nDisallow: /wiki/Wikipedia_talk:Conflict_of_interest/Noticeboard\\nDisallow: /wiki/Wikipedia_talk%3AConflict_of_interest/Noticeboard\\n#\\nDisallow: /wiki/Wikipedia:Long-term_abuse\\nDisallow: /wiki/Wikipedia%3ALong-term_abuse\\nDisallow: /wiki/Wikipedia_talk:Long-term_abuse\\nDisallow: /wiki/Wikipedia_talk%3ALong-term_abuse\\nDisallow: /wiki/Wikipedia:Long_term_abuse\\nDisallow: /wiki/Wikipedia%3ALong_term_abuse\\nDisallow: /wiki/Wikipedia_talk:Long_term_abuse\\nDisallow: /wiki/Wikipedia_talk%3ALong_term_abuse\\n#\\nDisallow: /wiki/Wikipedia:Wikiquette_assistance\\nDisallow: /wiki/Wikipedia%3AWikiquette_assistance\\n#\\nDisallow: /wiki/Wikipedia:Abuse_reports\\nDisallow: /wiki/Wikipedia%3AAbuse_reports\\nDisallow: /wiki/Wikipedia_talk:Abuse_reports\\nDisallow: /wiki/Wikipedia_talk%3AAbuse_reports\\nDisallow: /wiki/Wikipedia:Abuse_response\\nDisallow: /wiki/Wikipedia%3AAbuse_response\\nDisallow: /wiki/Wikipedia_talk:Abuse_response\\nDisallow: /wiki/Wikipedia_talk%3AAbuse_response\\n#\\nDisallow: /wiki/Wikipedia:Reliable_sources/Noticeboard\\nDisallow: /wiki/Wikipedia%3AReliable_sources/Noticeboard\\nDisallow: /wiki/Wikipedia_talk:Reliable_sources/Noticeboard\\nDisallow: /wiki/Wikipedia_talk%3AReliable_sources/Noticeboard\\n#\\nDisallow: /wiki/Wikipedia:Suspected_sock_puppets\\nDisallow: /wiki/Wikipedia%3ASuspected_sock_puppets\\nDisallow: /wiki/Wikipedia_talk:Suspected_sock_puppets\\nDisallow: /wiki/Wikipedia_talk%3ASuspected_sock_puppets\\n#\\nDisallow: /wiki/Wikipedia:Biographies_of_living_persons/Noticeboard\\nDisallow: /wiki/Wikipedia%3ABiographies_of_living_persons/Noticeboard\\nDisallow: /wiki/Wikipedia_talk:Biographies_of_living_persons/Noticeboard\\nDisallow: /wiki/Wikipedia_talk%3ABiographies_of_living_persons/Noticeboard\\nDisallow: /wiki/Wikipedia:Biographies_of_living_persons%2FNoticeboard\\nDisallow: /wiki/Wikipedia%3ABiographies_of_living_persons%2FNoticeboard\\nDisallow: /wiki/Wikipedia_talk:Biographies_of_living_persons%2FNoticeboard\\nDisallow: /wiki/Wikipedia_talk%3ABiographies_of_living_persons%2FNoticeboard\\n#\\nDisallow: /wiki/Wikipedia:Content_noticeboard\\nDisallow: /wiki/Wikipedia%3AContent_noticeboard\\nDisallow: /wiki/Wikipedia_talk:Content_noticeboard\\nDisallow: /wiki/Wikipedia_talk%3AContent_noticeboard\\n#\\nDisallow: /wiki/Template:Editnotices\\nDisallow: /wiki/Template%3AEditnotices\\n#\\nDisallow: /wiki/Wikipedia:Arbitration\\nDisallow: /wiki/Wikipedia%3AArbitration\\nDisallow: /wiki/Wikipedia_talk:Arbitration\\nDisallow: /wiki/Wikipedia_talk%3AArbitration\\n#\\nDisallow: /wiki/Wikipedia:Arbitration_Committee\\nDisallow: /wiki/Wikipedia%3AArbitration_Committee\\nDisallow: /wiki/Wikipedia_talk:Arbitration_Committee\\nDisallow: /wiki/Wikipedia_talk%3AArbitration_Committee\\n#\\nDisallow: /wiki/Wikipedia:Arbitration_Committee_Elections\\nDisallow: /wiki/Wikipedia%3AArbitration_Committee_Elections\\nDisallow: /wiki/Wikipedia_talk:Arbitration_Committee_Elections\\nDisallow: /wiki/Wikipedia_talk%3AArbitration_Committee_Elections\\n#\\nDisallow: /wiki/Wikipedia:Mediation_Committee\\nDisallow: /wiki/Wikipedia%3AMediation_Committee\\nDisallow: /wiki/Wikipedia_talk:Mediation_Committee\\nDisallow: /wiki/Wikipedia_talk%3AMediation_Committee\\n#\\nDisallow: /wiki/Wikipedia:Mediation_Cabal/Cases\\nDisallow: /wiki/Wikipedia%3AMediation_Cabal/Cases\\n#\\nDisallow: /wiki/Wikipedia:Requests_for_bureaucratship\\nDisallow: /wiki/Wikipedia%3ARequests_for_bureaucratship\\nDisallow: /wiki/Wikipedia_talk:Requests_for_bureaucratship\\nDisallow: /wiki/Wikipedia_talk%3ARequests_for_bureaucratship\\n#\\nDisallow: /wiki/Wikipedia:Administrator_review\\nDisallow: /wiki/Wikipedia%3AAdministrator_review\\nDisallow: /wiki/Wikipedia_talk:Administrator_review\\nDisallow: /wiki/Wikipedia_talk%3AAdministrator_review\\n#\\nDisallow: /wiki/Wikipedia:Editor_review\\nDisallow: /wiki/Wikipedia%3AEditor_review\\nDisallow: /wiki/Wikipedia_talk:Editor_review\\nDisallow: /wiki/Wikipedia_talk%3AEditor_review\\n#\\nDisallow: /wiki/Wikipedia:Article_Incubator\\nDisallow: /wiki/Wikipedia%3AArticle_Incubator\\nDisallow: /wiki/Wikipedia_talk:Article_Incubator\\nDisallow: /wiki/Wikipedia_talk%3AArticle_Incubator\\n#\\nDisallow: /wiki/Category:Noindexed_pages\\nDisallow: /wiki/Category%3ANoindexed_pages\\n#\\n# User sandboxes for modules and Template Styles are placed in these subpages for testing\\n#\\nDisallow: /wiki/Module:Sandbox\\nDisallow: /wiki/Module%3ASandbox\\nDisallow: /wiki/Template:TemplateStyles_sandbox\\nDisallow: /wiki/Template%3ATemplateStyles_sandbox\\n# Adminship and arbitration discussions are blocked from search engines, so blocking newer adminship processes (as of 2024) as well\\nDisallow: /wiki/Wikipedia:Administrator_recall\\nDisallow: /wiki/Wikipedia%3AAdministrator_recall\\nDisallow: /wiki/Wikipedia_talk:Administrator_recall\\nDisallow: /wiki/Wikipedia_talk%3AAdministrator_recall\\nDisallow: /wiki/Wikipedia:Administrator_elections\\nDisallow: /wiki/Wikipedia%3AAdministrator_elections\\nDisallow: /wiki/Wikipedia_talk:Administrator_elections\\nDisallow: /wiki/Wikipedia_talk%3AAdministrator_elections'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: Scrape a table of your choice\n",
    "# URL = \"your website with table\"\n",
    "# TODO: Complete the code to scrape a table from your chosen website\n",
    "\n",
    "# Sample solution with a different website\n",
    "# This is just an example - try finding your own table to scrape!\n",
    "exercise_url = (\n",
    "    \"https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)\"\n",
    ")\n",
    "\n",
    "# Check robots.txt first\n",
    "check_robots_txt(exercise_url)\n",
    "\n",
    "# Now scrape a table\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390cb588",
   "metadata": {},
   "source": [
    "## Dynamic Websites and JavaScript\n",
    "\n",
    "Many modern websites load content dynamically using JavaScript. This poses a challenge for basic web scraping because the requests library only fetches the initial HTML, not the content loaded by JavaScript.\n",
    "\n",
    "There are several approaches to scrape dynamic websites:\n",
    "\n",
    "1. **Find the API endpoints**: Often, dynamic content is loaded from JSON APIs\n",
    "2. **Use a headless browser**: Tools like Selenium or Playwright can run a browser engine\n",
    "3. **Use browser automation**: Control a real browser to interact with the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3b3290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts: 100\n",
      "\n",
      "First post:\n",
      "{\n",
      "  \"userId\": 1,\n",
      "  \"id\": 1,\n",
      "  \"title\": \"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\",\n",
      "  \"body\": \"quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\"\n",
      "}\n",
      "\n",
      "DataFrame of posts:\n",
      "   userId  id                                              title  \\\n",
      "0       1   1  sunt aut facere repellat provident occaecati e...   \n",
      "1       1   2                                       qui est esse   \n",
      "2       1   3  ea molestias quasi exercitationem repellat qui...   \n",
      "3       1   4                               eum et est occaecati   \n",
      "4       1   5                                 nesciunt quas odio   \n",
      "\n",
      "                                                body  \n",
      "0  quia et suscipit\\nsuscipit recusandae consequu...  \n",
      "1  est rerum tempore vitae\\nsequi sint nihil repr...  \n",
      "2  et iusto sed quo iure\\nvoluptatem occaecati om...  \n",
      "3  ullam et saepe reiciendis voluptatem adipisci\\...  \n",
      "4  repudiandae veniam quaerat sunt sed\\nalias aut...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Example using Selenium (requires installation: pip install selenium)\\nfrom selenium import webdriver\\nfrom selenium.webdriver.chrome.service import Service\\nfrom selenium.webdriver.chrome.options import Options\\nfrom webdriver_manager.chrome import ChromeDriverManager\\nfrom selenium.webdriver.common.by import By\\n\\n# Set up Chrome options\\nchrome_options = Options()\\nchrome_options.add_argument(\"--headless\")  # Run in headless mode (no UI)\\n\\n# Set up the driver\\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\\n\\n# Navigate to the page\\ndriver.get(\"https://quotes.toscrape.com/js/\")\\n\\n# Wait for JavaScript to load content\\nimport time\\ntime.sleep(2)\\n\\n# Now extract the content after JavaScript has run\\nquotes = driver.find_elements(By.CLASS_NAME, \"quote\")\\nfor i, quote in enumerate(quotes[:3]):\\n    text = quote.find_element(By.CLASS_NAME, \"text\").text\\n    author = quote.find_element(By.CLASS_NAME, \"author\").text\\n    print(f\"Quote {i+1}: {text}\")\\n    print(f\"Author: {author}\\n\")\\n\\n# Close the driver\\ndriver.quit()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Finding and using API endpoints that provide JSON data\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Many dynamic sites load data from JSON APIs\n",
    "# Let's fetch data from a sample JSON API\n",
    "api_url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "response = requests.get(api_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(f\"Number of posts: {len(data)}\")\n",
    "    print(\"\\nFirst post:\")\n",
    "    print(json.dumps(data[0], indent=2))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    posts_df = pd.DataFrame(data)\n",
    "    print(\"\\nDataFrame of posts:\")\n",
    "    print(posts_df.head())\n",
    "else:\n",
    "    print(f\"Failed to fetch data from API. Status code: {response.status_code}\")\n",
    "\n",
    "# Note: To use Selenium or Playwright, you'd need to install additional packages\n",
    "# The code below is commented out as it requires additional setup\n",
    "\n",
    "\"\"\"\n",
    "# Example using Selenium (requires installation: pip install selenium)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no UI)\n",
    "\n",
    "# Set up the driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# Navigate to the page\n",
    "driver.get(\"https://quotes.toscrape.com/js/\")\n",
    "\n",
    "# Wait for JavaScript to load content\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "# Now extract the content after JavaScript has run\n",
    "quotes = driver.find_elements(By.CLASS_NAME, \"quote\")\n",
    "for i, quote in enumerate(quotes[:3]):\n",
    "    text = quote.find_element(By.CLASS_NAME, \"text\").text\n",
    "    author = quote.find_element(By.CLASS_NAME, \"author\").text\n",
    "    print(f\"Quote {i+1}: {text}\")\n",
    "    print(f\"Author: {author}\\n\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c99c2",
   "metadata": {},
   "source": [
    "## Error Handling and Robust Scraping\n",
    "\n",
    "Web scraping can be unpredictable because websites change frequently. Here are some strategies for making your scraping more robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f80b4158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped: Quotes to Scrape\n",
      "Found 10 quotes\n",
      "Found 10 elements using selector: .quote .text\n"
     ]
    }
   ],
   "source": [
    "# Example: Robust scraping with error handling\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "\n",
    "def robust_scraper(url, max_retries=3, backoff_factor=2):\n",
    "    \"\"\"A robust web scraper with retry logic and error handling\"\"\"\n",
    "    headers = get_headers()\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Make the request with a timeout\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()  # Raise for 4XX/5XX status codes\n",
    "\n",
    "            # Parse the content\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            return soup\n",
    "\n",
    "        except RequestException as e:\n",
    "            wait_time = backoff_factor**retries\n",
    "            print(f\"Error: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            retries += 1\n",
    "\n",
    "    print(f\"Failed to retrieve {url} after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "soup = robust_scraper(\"https://quotes.toscrape.com\")\n",
    "if soup:\n",
    "    title = soup.title.text if soup.title else \"No title found\"\n",
    "    print(f\"Successfully scraped: {title}\")\n",
    "\n",
    "    # Defensive extraction using try-except\n",
    "    try:\n",
    "        quotes = soup.find_all(\"div\", class_=\"quote\")\n",
    "        print(f\"Found {len(quotes)} quotes\")\n",
    "    except AttributeError:\n",
    "        print(\"Could not extract quotes, page structure may have changed\")\n",
    "\n",
    "\n",
    "# Using CSS selectors as a more robust alternative\n",
    "def extract_with_css(soup, selector):\n",
    "    \"\"\"Extract data using CSS selectors with error handling\"\"\"\n",
    "    try:\n",
    "        elements = soup.select(selector)\n",
    "        return elements\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting using selector '{selector}': {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "if soup:\n",
    "    # Multiple selectors to try different approaches\n",
    "    selectors = [\n",
    "        \".quote .text\",  # Primary selector\n",
    "        \"div.quote span.text\",  # Alternative\n",
    "        \"[class='text']\",  # Attribute selector\n",
    "    ]\n",
    "\n",
    "    # Try each selector until one works\n",
    "    for selector in selectors:\n",
    "        elements = extract_with_css(soup, selector)\n",
    "        if elements:\n",
    "            print(f\"Found {len(elements)} elements using selector: {selector}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0061626",
   "metadata": {},
   "source": [
    "## Practical Exercises\n",
    "\n",
    "Let's put everything together with some practical exercises that combine the concepts we've learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd7788",
   "metadata": {},
   "source": [
    "### Exercise 1: Build a News Scraper\n",
    "\n",
    "Create a scraper that extracts headlines, summaries, and links from a news website.\n",
    "\n",
    "Requirements:\n",
    "1. Extract at least 10 headlines\n",
    "2. For each headline, get the summary/snippet and URL\n",
    "3. Save the results to a CSV file\n",
    "4. Be respectful of the website (add delays, proper headers)\n",
    "\n",
    "Suggested sites: BBC, Reuters, NPR (check robots.txt first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae9aa131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: News Scraper\n",
    "# Your code here...\n",
    "\n",
    "# Sample solution framework (you'll need to adapt this to your chosen site)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def news_scraper(url):\n",
    "    # TODO: Implement the news scraper\n",
    "    # 1. Send a request to the URL\n",
    "    # 2. Parse the HTML\n",
    "    # 3. Extract headlines, summaries, and links\n",
    "    # 4. Return the data\n",
    "\n",
    "    # Placeholder for the solution\n",
    "    return {\"headlines\": [], \"summaries\": [], \"links\": []}\n",
    "\n",
    "\n",
    "# Test your scraper\n",
    "# news_data = news_scraper('https://your-chosen-news-site.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59cd5e",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a Web Monitor\n",
    "\n",
    "Build a tool that monitors a webpage for changes and sends a notification when changes are detected.\n",
    "\n",
    "Requirements:\n",
    "1. Take a URL as input\n",
    "2. Periodically check the webpage (e.g., every hour)\n",
    "3. Compare with the previous version to detect changes\n",
    "4. Print a notification when changes are detected\n",
    "\n",
    "Bonus: Save the history of changes to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87c8c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Web Monitor\n",
    "# Your code here...\n",
    "\n",
    "# Sample solution framework\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def get_page_content(url):\n",
    "    # TODO: Fetch the page content\n",
    "    pass\n",
    "\n",
    "\n",
    "def generate_content_hash(content):\n",
    "    # TODO: Generate a hash of the content to detect changes\n",
    "    pass\n",
    "\n",
    "\n",
    "def monitor_webpage(url, check_interval=3600):\n",
    "    # TODO: Implement the monitoring logic\n",
    "    # 1. Get the initial content\n",
    "    # 2. Periodically check for new content\n",
    "    # 3. Compare with previous content\n",
    "    # 4. Notify if changes are detected\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your monitor (reduced interval for testing)\n",
    "# monitor_webpage('https://example.com', check_interval=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c702f3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've learned how to:\n",
    "1. Import and use web scraping libraries\n",
    "2. Fetch HTML content from a website\n",
    "3. Parse the HTML using BeautifulSoup\n",
    "4. Extract specific data from the parsed HTML\n",
    "5. Handle pagination to scrape data across multiple pages\n",
    "6. Scrape structured data like tables\n",
    "7. Handle dynamic websites with JavaScript content\n",
    "8. Implement error handling for robust scraping\n",
    "9. Apply these concepts in practical exercises\n",
    "\n",
    "Remember to always be respectful when scraping websites by:\n",
    "- Reading and following the website's robots.txt file\n",
    "- Adding delays between requests to avoid overloading the server\n",
    "- Identifying your scraper with an appropriate user agent\n",
    "- Only scraping data that is publicly available and legal to scrape\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Requests Library Documentation](https://docs.python-requests.org/en/latest/)\n",
    "- [Selenium Documentation](https://www.selenium.dev/documentation/)\n",
    "- [Web Scraping Ethics and Legality](https://www.scrapingbee.com/blog/web-scraping-legal/)\n",
    "\n",
    "Happy scraping!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
